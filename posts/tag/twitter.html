<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns#">
<head>
        <title>Michal Zimmermann | tag: twitter</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link href="https://www.zimmi.cz/posts/atom.xml" type="application/atom+xml" rel="alternate" title="Michal Zimmermann Full Atom Feed" />
        <link href="https://www.zimmi.cz/posts/feed.xml" type="application/rss+xml" rel="alternate" title="Michal Zimmermann Full RSS Feed" />
        <link rel="stylesheet" href="https://www.zimmi.cz/posts/theme/css/style.min.css?d5d1e141">
</head>

<body id="index" class="home">
    <nav class="top-bar card-1" data-topbar role="navigation">
        <ul class="title-area">
            <li class="name">
                <h1><a href="/posts">Home</a></h1>
            </li>
     <!-- Remove the class "menu-icon" to get rid of menu icon. Take out "Menu" to just have icon alone -->
            <li class="toggle-topbar menu-icon"><a href="#"><span></span></a></li>
        </ul>
        <section class="top-bar-section">
    <!-- Right Nav Section -->
            <ul class="right">
                <li><a href="https://www.zimmi.cz/posts/categories">Categories</a></li>
                <li><a href="https://www.zimmi.cz/posts/tags">Tags</a></li>
                <li><a href="https://www.zimmi.cz/posts/feed.xml">RSS feed</a></li>
            </ul>
        </section>
    </nav>
        <div class="row">
          <div class="large-12 columns header">
            <h1><a href="/posts">Michal Zimmermann</a> <small>Pieces of knowledge from the world of GIS.</small></h1>
          </div>
        </div>
        <div class="row">
            <div class="large-12 columns">
<h2 class="text-center">Articles tagged with twitter tag</h2>

    <div class="panel card card-1">
        <div class="post-name">
            <h1><a href="https://www.zimmi.cz/posts/2015/twitter-rest-api-data-mining-on-openshift-part-ii/" rel="bookmark" title="Permalink to Twitter REST API Data Mining on OpenShift (Part II)">Twitter <span class="caps">REST</span> <span class="caps">API</span> Data Mining on OpenShift (Part <span class="caps">II</span>)</a></h1>
            <small>Written on Dec 6, 2015
        and marked as
        <a href="https://www.zimmi.cz/posts/tag/javascript.html">javascript</a>,         <a href="https://www.zimmi.cz/posts/tag/openshift.html">openshift</a>,         <a href="https://www.zimmi.cz/posts/tag/twitter.html">twitter</a>        | <a href="https://www.zimmi.cz/posts/category/development.html">development</a>
        </small>
        <section>
            <p>Last time I described <a href="https://www.zimmi.cz/posts/2015/twitter-rest-api-data-mining-on-openshift-part-i/">the setup of my OpenShift Twitter crawler</a> and let it running and downloading data. It&#8217;s been more than two months since I started and I got interesting amount of data. I also made a simple <span class="caps">ETL</span> process to load it into my local PostGIS database, which I&#8217;d like to cover in this&nbsp;post.</p>
<h2>Extract&nbsp;data</h2>
<p>Each day is written to the separate sqlite file with a name like <code>tw_day_D_M_YYYY</code>. <code>Bash</code> is used to gzip all the files before downloading them from&nbsp;OpenShift.</p>
<div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

ssh openshift <span class="s">&lt;&lt; EOF</span>
<span class="s">    cd app-root/data</span>
<span class="s">    tar czf twitter.tar.gz *.db</span>
<span class="s">EOF</span>

scp openshift:/var/lib/openshift/55e487587628e1280b0000a9/app-root/data/twitter.tar.gz ./data
<span class="nb">cd</span> data <span class="o">&amp;&amp;</span>
tar -xzf twitter.tar.gz <span class="o">&amp;&amp;</span>
<span class="nb">cd</span> -

<span class="nb">echo</span> <span class="s2">&quot;Extract done&quot;</span>
</pre></div>


<h2>Transform&nbsp;data</h2>
<p>The transformation part operates on downloaded files and merges them into one big <span class="caps">CSV</span> file. That&#8217;s pretty straightforward. Note that&#8217;s quite simple with sqlite flags, some <code>sed</code> and <code>tail</code> commands.</p>
<div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

rm -rf ./data/csv
mkdir ./data/csv

<span class="k">for</span> db in ./data/*.db<span class="p">;</span> <span class="k">do</span>
    <span class="nv">FILENAME</span><span class="o">=</span><span class="k">$(</span>basename <span class="nv">$db</span><span class="k">)</span>
    <span class="nv">DBNAME</span><span class="o">=</span><span class="si">${</span><span class="nv">FILENAME</span><span class="p">%%.db</span><span class="si">}</span>
    <span class="nv">CSVNAME</span><span class="o">=</span><span class="nv">$DBNAME</span>.csv
    <span class="nb">echo</span> <span class="s2">&quot;</span><span class="nv">$DBNAME</span><span class="s2"> to csv...&quot;</span>
    sqlite3 -header -csv <span class="nv">$db</span> <span class="s2">&quot;select * from </span><span class="nv">$DBNAME</span><span class="s2">;&quot;</span> &gt; ./data/csv/<span class="nv">$CSVNAME</span>
<span class="k">done</span>

<span class="nb">cd</span> ./data/csv
touch tweets.csv
<span class="nb">echo</span> <span class="k">$(</span>sed -n 1p <span class="k">$(</span>ls -d -1 *.csv <span class="p">|</span> head -n 1<span class="k">))</span> &gt; tweets.csv <span class="c1"># get column names</span>

<span class="k">for</span> csv in tw_*.csv<span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="nv">$csv</span>
    tail -n +2 <span class="nv">$csv</span> &gt;&gt; tweets.csv <span class="c1"># get all lines without the first one</span>
<span class="k">done</span>
</pre></div>


<h2>Load&nbsp;data</h2>
<p>In the last step, the data is loaded with <span class="caps">SQL</span> <code>\copy</code> command.</p>
<div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">export</span> <span class="nv">PG_USE_COPY</span><span class="o">=</span>YES

<span class="nv">DATABASE</span><span class="o">=</span>mzi_dizertace
<span class="nv">SCHEMA</span><span class="o">=</span>dizertace
<span class="nv">TABLE</span><span class="o">=</span>tweets

psql <span class="nv">$DATABASE</span> <span class="s">&lt;&lt; EOF</span>
<span class="s">    DROP TABLE IF EXISTS $SCHEMA.$TABLE;</span>
<span class="s">    CREATE UNLOGGED TABLE $SCHEMA.$TABLE (id text, author text, author_id text, tweet text, created_at text, lon float, lat float, lang text);</span>
<span class="s">    \copy $SCHEMA.$TABLE FROM &#39;data/csv/tweets.csv&#39; CSV HEADER DELIMITER &#39;,&#39;</span>
<span class="s">    ALTER TABLE $SCHEMA.$TABLE ADD COLUMN wkb_geometry geometry(POINT, 4326);</span>
<span class="s">    UPDATE $SCHEMA.$TABLE SET wkb_geometry = ST_SetSRID(ST_MakePoint(lon, lat), 4326);</span>
<span class="s">    CREATE INDEX ${TABLE}_geom_idx ON $SCHEMA.$TABLE USING gist(wkb_geometry);</span>
<span class="s">    COMMIT;</span>
<span class="s">EOF</span>
</pre></div>


<h2>First&nbsp;statistics</h2>
<p>Some interesting charts and numbers&nbsp;follow.</p>
<p class="text-center"><img title="Top 100 Twitter users in the Czech Republic" src="https://www.zimmi.cz/posts/assets/twitter-rest-api-data-mining-on-openshift-part-ii/authors.png" class="img-responsive centered"></p>

<p class="text-center"><img title="When people tweet in the Czech Republic" src="https://www.zimmi.cz/posts/assets/twitter-rest-api-data-mining-on-openshift-part-ii/hours.png" class="img-responsive centered"></p>

<p class="text-center"><img title="Languages on Twitter in the Czech Republic" src="https://www.zimmi.cz/posts/assets/twitter-rest-api-data-mining-on-openshift-part-ii/languages.png" class="img-responsive centered"></p>
        </section>
        </div>
    </div>

    <div class="panel card card-1">
        <div class="post-name">
            <h1><a href="https://www.zimmi.cz/posts/2015/twitter-rest-api-data-mining-on-openshift-part-i/" rel="bookmark" title="Permalink to Twitter REST API Data Mining on OpenShift (Part I)">Twitter <span class="caps">REST</span> <span class="caps">API</span> Data Mining on OpenShift (Part&nbsp;I)</a></h1>
            <small>Written on Nov 6, 2015
        and marked as
        <a href="https://www.zimmi.cz/posts/tag/javascript.html">javascript</a>,         <a href="https://www.zimmi.cz/posts/tag/openshift.html">openshift</a>,         <a href="https://www.zimmi.cz/posts/tag/twitter.html">twitter</a>        | <a href="https://www.zimmi.cz/posts/category/development.html">development</a>
        </small>
        <section>
            <p>More than a year ago I wrote about <a href="http://www.zimmi.cz/posts/2014/analyzing-twitter-languages-with-streaming-api/">analyzing Twitter languages with Streaming <span class="caps">API</span></a>. Back then I kept my laptop running for a week to download data. Not a comfortable way, especially if you decide to get more data. One year uptime doesn&#8217;t sound like anything you want to be part of. <a href="https://www.openshift.com/">OpenShift</a> by Red Hat seems to be almost perfect replacement.&nbsp;Almost.</p>
<h2>OpenShift&nbsp;setup</h2>
<p>I started with Node.js application running on one small gear. Once running, you can easily <code>git push</code> the code to your OpenShift repo and login via <span class="caps">SSH</span>. I quickly found simple copy-pasting my local solution wasn&#8217;t going to work. and fixed it with some minor tweaks. That&#8217;s where the fun&nbsp;begins&#8230;</p>
<blockquote>
<p>I based the downloader on Node.js a year ago. Until now I still don&#8217;t get how that piece of software works. Frankly, I don&#8217;t really care as long as it&nbsp;works.</p>
</blockquote>
<h3>Pitfalls</h3>
<p>If your application doesn&#8217;t generate any traffic, <strong>OpenShift turns it off</strong>. It wakes up once someone visits again. I had no idea about that and spent some time trying to stop that behavior. Obviously, I could have scheduled a cron job on my laptop pinging it every now and then. Luckily, OpenShift can run cron jobs itself. All you need is to embed a cron cartridge into the running application (and install a bunch of ruby dependencies&nbsp;beforehand).</p>
<div class="highlight"><pre><span></span>rhc cartridge add cron-1.4 -a app-name
</pre></div>


<p>Then create <code>.openshift/cron/{hourly,daily,weekly,monthly}</code> folder in the git repository and put your script running a simple curl command into one of&nbsp;those.</p>
<div class="highlight"><pre><span></span>curl http://social-zimmi.rhcloud.com &gt; /dev/null
</pre></div>


<p>Another problem was just around the corner. Once in a while, the app stopped writing data to the database without saying a word. What helped was restarting it - the only automatic way to do so being a <code>git push</code> command. Sadly, I haven&#8217;t found a way to restart the app from within itself; it probably can&#8217;t be&nbsp;done.</p>
<p>When you <code>git push</code>, the gear stops, builds, deploys and restarts the app. By using hot deployment you can minimize the downtime. Just put the <code>hot_deploy</code> file into <code>.openshift/markers</code> folder.</p>
<div class="highlight"><pre><span></span>git commit --allow-empty -m <span class="s2">&quot;Restart gear&quot;</span> <span class="o">&amp;&amp;</span> git push
</pre></div>


<p>This solved the problem until I realize that <strong>every restart deleted all the data</strong> collected so far. If your data are to stay safe and sound, <strong>save them in <code>process.env.OPENSHIFT_DATA_DIR</code></strong> (which is <code>app-root/data</code>).</p>
<h3>Anacron to the&nbsp;rescue</h3>
<p>How do you push an empty commit once a day? With cron of course. Even better, <strong>anacron</strong>.</p>
<div class="highlight"><pre><span></span>mkdir ~/.anacron
<span class="nb">cd</span> ~/.anacron
mkdir cron.daily cron.weekly cron.monthly spool etc

cat <span class="s">&lt;&lt;EOT &gt; ~/.anacron/etc/anacrontab</span>

<span class="s">SHELL=/bin/sh</span>
<span class="s">PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/$HOME/bin</span>
<span class="s">HOME=$HOME</span>
<span class="s">LOGNAME=$USER</span>

<span class="s">1 5  daily-cron nice run-parts --report $HOME/.anacron/cron.daily</span>
<span class="s">7 10 weekly-cron nice run-parts --report $HOME/.anacron/cron.weekly</span>
<span class="s">@monthly 15 monthly-cron nice run-parts --report $HOME/.anacron/cron.monthly</span>

<span class="s">EOT</span>

cat <span class="s">&lt;&lt;EOT &gt;&gt; ~/.zprofile # I use zsh shell</span>
<span class="s">rm -f $HOME/.anacron/anacron.log</span>
<span class="s">/usr/sbin/anacron -t /home/zimmi/.anacron/etc/anacrontab -S /home/zimmi/.anacron/spool &amp;&gt; /home/zimmi/.anacron/anacron.log</span>

<span class="s">EOT</span>
</pre></div>


<p>Anacron is to laptop what cron is to 24/7 running server. It just runs automatic jobs when the laptop is running. If it&#8217;s not and the job should be run, it runs it once the <span class="caps">OS</span> boots. Brilliant&nbsp;idea.</p>
<p>It runs the following code for me to keep the app writing data to the&nbsp;database.</p>
<div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nv">workdir</span><span class="o">=</span><span class="s1">&#39;/home/zimmi/documents/zimmi/dizertace/social&#39;</span>
<span class="nv">logfile</span><span class="o">=</span><span class="nv">$workdir</span>/restart-gear.log
date &gt; <span class="nv">$logfile</span>

<span class="o">{</span> 
<span class="nv">HOME</span><span class="o">=</span>/home/zimmi
<span class="nb">cd</span> <span class="nv">$workdir</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
git merge origin/master <span class="o">&amp;&amp;</span> <span class="se">\</span>
git commit --allow-empty -m <span class="s2">&quot;Restart gear&quot;</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
git push <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="nb">echo</span> <span class="s2">&quot;Success&quot;</span> <span class="p">;</span> 
<span class="o">}</span> &gt;&gt; <span class="nv">$logfile</span> 2&gt;<span class="p">&amp;</span>1
</pre></div>


<p><strong><span class="caps">UPDATE</span>:</strong> Spent a long time debugging the &#8220;Permission denied (publickey).&#8221;-like errors. What seems to help&nbsp;is:</p>
<ol>
<li>Use id_rsa instead of any other <span class="caps">SSH</span>&nbsp;key</li>
<li>Put a new entry into the <code>~/.ssh/config</code> file</li>
</ol>
<p>I don&#8217;t know which one did the magic&nbsp;though.</p>
<p>I&#8217;ve been harvesting Twitter for a month with about 10-15K tweets a day (only interested in the Czech Republic). 
<sup>1</sup>&frasl;<sub>6</sub> to <sup>1</sup>&frasl;<sub>5</sub> of them is located with latitude and longitude. More on this next&nbsp;time.</p>
        </section>
        </div>
    </div>

    <div class="panel card card-1">
        <div class="post-name">
            <h1><a href="https://www.zimmi.cz/posts/2014/analyzing-twitter-languages-with-streaming-api/" rel="bookmark" title="Permalink to Analyzing Twitter Languages With Streaming API">Analyzing Twitter Languages With Streaming <span class="caps">API</span></a></h1>
            <small>Written on Sep 2, 2014
        and marked as
        <a href="https://www.zimmi.cz/posts/tag/javascript.html">javascript</a>,         <a href="https://www.zimmi.cz/posts/tag/leaflet.html">leaflet</a>,         <a href="https://www.zimmi.cz/posts/tag/twitter.html">twitter</a>        | <a href="https://www.zimmi.cz/posts/category/development.html">development</a>
        </small>
        <section>
            <p>I am writing a diploma thesis focused on extracting spatial data from social networks. I have been working mainly with Twitter <span class="caps">API</span> and results I have got so far look really promising. This post was written as a reaction to many retweets I got when I shared one of my visualizations. It aims to make it clear how to connect to Twitter Streaming <span class="caps">API</span> using <a href="http://nodejs.org/">node.js</a>, <a href="http://leafletjs.com/">Leaflet</a> and <a href="http://sqlite.org/">SQLite</a> and retrieve tweets to analyze them&nbsp;later.</p>

<p>If you have any further questions after reading this paper, feel free to contact me via <a href="https://twitter.com/zimmicz">Twitter</a> or <a href="mailto:zimmicz@gmail.com">e-mail</a>. I must say right here <strong>that the code will be shared as well as the map</strong>, but there are still some bugs/features I would like to&nbsp;remove/add.</p>

<p><small>On a side note: I have been studying cartography and <span class="caps">GIS</span> for the last five years at Masaryk University in Brno, Czech Republic. I am mostly interested in ways computers can make data handling easier. I&nbsp;like to code in Python.</small></p>

<h3>Using Twitter Streaming <span class="caps">API</span></h3>

<p>As you probably know, Twitter offers three different&nbsp;APIs:</p>

<ul>
<li><span class="caps">REST</span> <span class="caps">API</span> which is obviously RESTful. You can access almost every piece of information on Twitter with this one: tweets, users, places, retweets,&nbsp;followers&#8230;</li>
<li>Search <span class="caps">API</span> used for getting search results. You can customize these by sending parameters with your&nbsp;requests.</li>
<li><strong>Streaming <span class="caps">API</span></strong> which I am going to tell you about. It is really different, as (again, obviously) it keeps streaming tweets from the time you connect to the server. This means, once the connection is made, it has to stay open as long as you want tweets coming to you. The important thing here is that you get real time tweets delivered to you via this <abbr title="Twitter only delivers a sample of tweets, not the whole traffic.">stream</abbr>, which implies you cannot use this <span class="caps">API</span> to get tweets already&nbsp;tweeted.</li>
</ul>

<p><strong>To sum it up: You get a small sample of tweets in a real time as long as the connection to the server stays&nbsp;open.</strong></p>

<h3>What you&nbsp;need</h3>

<p>To use any of the Twitter APIs, you need to authenticate you (or your app) against Twitter via OAuth protocol. To be able to do so, you need a Twitter account, because only then you can <a href="https://dev.twitter.com/">create apps</a>, obtain access tokens and get authenticated for <span class="caps">API</span>&nbsp;use.</p>

<p>And then, obviously, you need something to connect to server with. I chose <strong>node.js</strong> because it seemed as a good tool to keep connection alive. I have also been interested in this technology for the couple of months but never really had a task to use it&nbsp;for.</p>

<p>The good thing about node.js is that it comes with lots of handy libraries. You get <strong>socket.io</strong> for streaming, <strong>ntwitter</strong> for using Twitter <span class="caps">API</span> and <strong>sqlite3</strong> for working with SQLite&nbsp;databases.</p>

<p>You need something to store the data in also. As mentioned, I picked SQLite for this task. It is lightweight, does not need server nor configuration to run, just what I was looking for. Seems we are set to go,&nbsp;right?</p>

<h3>Filtering the&nbsp;data</h3>

<p>I guess none of you is interested in obtaining random tweets from around the world, neither was I. I live in the Czech republic and that is the area I want to get tweets from.&nbsp;How?</p>

<p>It is fairly simple, you tell Twitter with the <code>locations</code> parameter of <a href="https://dev.twitter.com/docs/api/1.1/post/statuses/filter"><code>statuses/filter</code></a> resource. This parameter specifies a set of bounding boxes to&nbsp;track.</p>

<p><strong>To sum it up: you connect to the server and tell it you just want to get tweets from the area you specified with the <code>locations</code> parameter. The server understands and keeps you&nbsp;posted.</strong></p>

<h4>Is it that&nbsp;simple?</h4>

<p>No. Twitter decides whether to post you the tweet or not according to what the value of coordinates field is. It goes like&nbsp;this:</p>

<ol>
<li>If the <code>coordinates</code> field is not empty, it gets tested against the bounding box. If it matches, it is sent to the&nbsp;stream.</li>
<li>If the <code>coordinates</code> field is empty, but the <code>place</code> field is not, it is the <code>place</code> field that gets checked. If if it by any extent intersects the bounding box, it is sent to the&nbsp;stream.</li>
<li>If both of the fields are empty, nothing is&nbsp;sent.</li>
</ol>

<p>I decided to throw away the tweets with the empty <code>coordinates</code> field, because the accuracy of the value specified in the place field can be generally considered very low and insufficient for my purposes. You still need to account for position inaccuracies of users&#8217; devices though, however that is not something that we can deal with. <em>Let us just assume that geotagged tweets are&nbsp;accurate.</em></p>

<div class="text-center"><img src="https://www.zimmi.cz/posts/assets/analyzing-twitter-languages-with-streaming-api/cr.png" width="50%" height="50%" title="Geotagged tweets" class="img-rounded"><p><strong>Figure:</strong> Twitter seems not to be very accurate when matching tweets against bounding&nbsp;box.</p></div>

<p>Although, as you can see in the picture, they are not. Or they are, but Twitter is not good at telling so. Besides that, none of the countries in the world is shaped like a rectangle and we would need to clip the data anyway. That is where SQLite comes in, because I have been saving incoming tweets right into the&nbsp;database.</p>

<p>If you use any <span class="caps">GUI</span> manager (sqlitebrowser for Linux is just fine), you can easily export your data to the <span class="caps">CSV</span> file, load it into <span class="caps">QGIS</span>, clip it with Natural Earth countries shapefile and save them to the GeoJSON file. It is just a matter of few JavaScript lines of code to put GeoJSON on a Leaflet&nbsp;map.</p>

<h3>Displaying the&nbsp;data</h3>

<p>Once a GeoJSON file is ready, it can be used for making an appealing viz to get a sense of what may be called &#8220;nationalities spatial patterns&#8221;. The <code>lang</code> field (stored in the database, remember?) of every tweet is used to colour the marker accordingly. Its value represents a two-letter language code as specified in <span class="caps">ISO</span> 639-1&nbsp;document.</p>

<p>However, as those codes are guessed by Twitter&#8217;s language algorithms, they are prone to error. There are actually three scenarios we might be&nbsp;facing:</p>

<ol>
<li>User tweets in the same language as used in the Twitter&nbsp;account.</li>
<li>User tweets in his/her mother language, but has set different Twitter account&nbsp;language.</li>
<li>User does not tweet in his/her mother language, but has it set as a Twitter account&nbsp;language.</li>
</ol>

<p>We basically have to deal with 2) and 3), because 1) means we can be pretty sure what nationality the user is. Sadly though, I have not found an easy way to tell which one of these two we came across, thus which language settings should be prioritized. I made an arbitrary decision to prioritize the language the tweet was written in, based on assumption that <strong>the most of the users tweet in their mother language</strong>. No matter what you do, the data is still going to be biased by automatically generated tweets, especially ones sent by Foursquare saying &#8220;I&#8217;m at @WhateverBarItIs (http://someurl.co)&#8221;. It works fine for the strange languages like Russian and Arabic&nbsp;though.</p>

<p>From Jan 2 to Jan 4 this year 5,090 tweets were collected. Leaflet is becoming a little sluggish without clustering turned on displaying all of them. Plans are to let the collection run until Jan 7 and then put all the tweets on the map. I guess that might be around 10,000 geotagged tweets by that&nbsp;time.</p>

<p>I am definitely willing to share the <abbr title="Do not expect much, it was my first time with node.js">code</abbr> and the final viz. Meanwhile, you can have a look at the screenshot on picture [*]. I have already implemented nationality switch (legend items are clickable) and I would like to add a day/night switch to see whether there are any differences between the peoples&#8217;&nbsp;behaviour. </p>

<div class="text-center"><img width="60%" height="60%" src="https://www.zimmi.cz/posts/assets/analyzing-twitter-languages-with-streaming-api/screenshot.png" title="Final geoviz using Leaflet" class="img-rounded"><p><strong>Figure:</strong> Final map screenshot. A legend is used to turn nationalities on and off. You are looking at Prague by the&nbsp;way.</p></div>

<p>Obviously the most tweets were sent from the most populated places, e.g. Prague, Brno,&nbsp;Ostrava. </p>
        </section>
        </div>
    </div>

    <div class="panel card card-1">
        <div class="post-name">
            <h1><a href="https://www.zimmi.cz/posts/2014/going-3d-with-space-time-cube/" rel="bookmark" title="Permalink to Going 3D With Space Time Cube">Going 3D With Space Time&nbsp;Cube</a></h1>
            <small>Written on Sep 2, 2014
        and marked as
        <a href="https://www.zimmi.cz/posts/tag/python.html">python</a>,         <a href="https://www.zimmi.cz/posts/tag/twitter.html">twitter</a>        | <a href="https://www.zimmi.cz/posts/category/development.html">development</a>
        </small>
        <section>
            <p>Seeing <a href="http://anitagraser.com/2012/08/05/space-time-cubes-exploring-twitter-streams-3/">Anita&#8217;s space-time cube</a> back in 2013 was a moment of <em>woooow</em> for me. I&#8217;ve been interested in unusual ways of displaying data ever since I started studying <span class="caps">GIS</span> and this one was just great. <em>How the hell did she make it?!</em>, I thought back&nbsp;then.</p>

<p>And I asked her, we had a little e-mail conversation and that was it. I got busy and had to postpone my attemps to create that viz until I dove into my diploma thesis. So&hellip;here you&nbsp;go.</p>

<h3>Recipe</h3>

<p>What you need&nbsp;is:</p>

<ul>
<li><strong><a href="https://github.com/jdf/processing.py">processing.py</a></strong> which is a Python port of <a href="http://processing.org/">processing</a>&nbsp;environment.</li>
<li>A <strong>basemap</strong> that fits the extent you are about to show in the viz. I recommend <span class="caps">QGIS</span> for obtaining an&nbsp;image.</li>
<li>A <strong><span class="caps">JSON</span> file</strong> with tweets you got via <a href="/2014/analyzing-twitter-languages-with-streaming-api/">Twitter <span class="caps">REST</span> <span class="caps">API</span></a> (yes, the viz was made to display&nbsp;tweets).</li>
<li>A <strong>python script</strong> I&nbsp;wrote.</li>
</ul>

<h3>How to make it&nbsp;delicious</h3>

<p>First things first, you need to add a <code>timestamp</code> property to tweets you want to show (with the following Python code). <code>created_at</code> param is a datetime string like <code>Sat Jun 22 21:30:42 +0000 2013</code> of every tweet in a loop. As a result you get a number of seconds since&nbsp;1.1.1970. </p>

<pre><code>def string_to_timestamp(created_at):
    """Return the timestamp from created_at object."""
    locale.setlocale(locale.LC_TIME, 'en_US.utf8')
    created_at = created_at.split(' ')
    created_at[1] = str(strptime(created_at[1], '%b').tm_mon)
    timestamp = strptime(' '.join(created_at[i] for i in [1,2,3,5]), '%m %d %H:%M:%S %Y') # returns Month Day Time Year
    return mktime(timestamp)
</code></pre>

<p>As you probably guess, the <code>timestamp</code> property is the one we&#8217;re gonna display on the vertical axis. <strong>You definitely want the tweets to be sorted chronologically in your <span class="caps">JSON</span>&nbsp;file!</strong></p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-
#avconv -i frame-%04d.png -r 25 -b 65536k  video.mp4

from peasy import PeasyCam
import json

basemap = None
tweets = []
angle = 0

def setup():
    global basemap
    global tweets

    size(1010, 605, P3D)

    data = loadJSONArray('./tweets.json')
    count = data.size()

    last = data.getJSONObject(data.size()-1).getFloat('timestamp')
    first = data.getJSONObject(0).getFloat('timestamp')

    for i in range(0, count):
        lon = data.getJSONObject(i).getJSONObject('coordinates').getJSONArray('coordinates').getFloat(0)
        lat = data.getJSONObject(i).getJSONObject('coordinates').getJSONArray('coordinates').getFloat(1)
        time = data.getJSONObject(i).getFloat('timestamp')

        x = map(lon, -19.68624620368202116, 58.92453879754536672, 0, width)
        y = map(time, first, last, 0, 500)
        z = map(lat, 16.59971950210866964, 63.68835804244784526, 0, height)

        tweets.append({'x': x, 'y': y, 'z': z})

    basemap = loadImage('basemap.png')

    cam = PeasyCam(this,53,100,-25,700)
    cam.setMinimumDistance(1)
    cam.setMaximumDistance(1500)

def draw():
    global basemap
    global tweets
    global angle

    background(0)

    # Uncomment to rotate the cube
    """if angle &lt; 360:
        rotateY(radians(angle))
        angle += 1
    else:
        angle = 360 - angle"""

    # box definition
    stroke(150,150,150)
    strokeWeight(.5)
    noFill()
    box(1010,500,605)


    # basemap definition
    translate(-505,250,-302.5)
    rotateX(HALF_PI)
    image(basemap,0,0)

    for i in range(0, len(tweets)):
        strokeWeight(.5)
        stroke(255,255,255)
        line(tweets[i].get('x'), height-tweets[i].get('z'), tweets[i].get('y'), tweets[i].get('x'), height-tweets[i].get('z'), 0)

        strokeWeight(5)
        stroke(255,0,0)
        point(tweets[i].get('x'), height-tweets[i].get('z'), tweets[i].get('y'))

        strokeWeight(2)
        stroke(255,255,255)
        point(tweets[i].get('x'), height-tweets[i].get('z'), 0)
        lrp = map(i, 0, len(tweets), 0, 1)
        frm = color(255,0,0)
        to = color(0,0,255)
        if i &lt; len(tweets)-1:
            strokeWeight(1)
            stroke(lerpColor(frm,to,lrp))
            line(tweets[i].get('x'), height-tweets[i].get('z'), tweets[i].get('y'), tweets[i+1].get('x'), height-tweets[i+1].get('z'), tweets[i+1].get('y'))

    # Uncomment to capture the screens
    """if frameCount &gt; 360:
        noLoop()
    else:
        saveFrame('screens/frame-####.png')"""
</code></pre>

<p>You should be most interested in these&nbsp;lines:</p>

<pre><code>x = map(lon, -19.68624620368202116, 58.92453879754536672, 0, width)
y = map(time, first, last, 0, 500)
z = map(lat, 16.59971950210866964, 63.68835804244784526, 0, height)
</code></pre>

<p><img src="http://www.processing.org/tutorials/p3d/imgs/coordinatesystem.png" title="Processing coordinate system" class="img-rounded pull-left">They define how coordinates inside the cube should be computed. As you see, <code>x</code> is the result of mapping longitudinal extent of our area to the width of cube, the same happens to <code>z</code> and latitude, and to <code>y</code> (but here we map time, not&nbsp;coordinates).</p>

<p>The bounding box used in those computations is the bounding box of the basemap. Interesting thing about Processing and its 3D environment is how it defines the beginning of the coordinate system. As you can see on the left, it might be slighty different from what you could expect. That&#8217;s what you need to be careful&nbsp;about.</p>

<h3>How does it&nbsp;look</h3>

<iframe width="420" height="315" src="//www.youtube.com/embed/4jl6-qOiSAE?rel=0" frameborder="0" allowfullscreen></iframe>
        </section>
        </div>
    </div>

<div class="pagination row">
    <div class="large-6 columns">
    </div>
    <div class="large-6 columns">
    </div>
    <!-- <p class="paginator">
    </p> -->
</div>            </div>
        </div>
        <div class="row">
            <div class="large-12 columns footer">
                <footer>
                    <address>
                    Written by <a href="http://www.zimmi.cz">Michal Zimmermann</a>.
                    Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                    which takes great advantage of <a href="http://python.org">Python</a>.
                    </address><!-- /#about -->
                </footer><!-- /#contentinfo -->
            </div>
        </div>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-43432739-2']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
<script src="https://www.zimmi.cz/posts/theme/js/packed.js?8bee1467"></script>
<script>
$(document).foundation();
</script>
</body>
</html>